import streamlit as st
import pandas as pd
import folium
from folium.plugins import HeatMap
from geopy.geocoders import Nominatim
from google import genai
from google.genai import types
import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import os
import json
from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
import io
from datetime import datetime, timedelta
import time
import math
import re

# --- è¨­å®šã¨APIåˆæœŸåŒ– ---
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]
try:
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
    service_account_json_str = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
    if not service_account_json_str:
        st.error("ç’°å¢ƒå¤‰æ•° 'GOOGLE_SERVICE_ACCOUNT_JSON' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()
    
    service_account_info = json.loads(service_account_json_str)
    credentials = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
    gc = gspread.authorize(credentials)
    drive_service = build('drive', 'v3', credentials=credentials)

    SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
    DRIVE_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")

    if not SPREADSHEET_ID or not DRIVE_FOLDER_ID:
        st.error("ç’°å¢ƒå¤‰æ•° 'SPREADSHEET_ID' ã¾ãŸã¯ 'DRIVE_FOLDER_ID' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

except (json.JSONDecodeError, Exception) as e:
    st.error(f"Google APIã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# Gemini
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    st.error("ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()
# client ã®åˆæœŸåŒ–ã¯æˆåŠŸã—ãŸå ´åˆã®ã¿è¡Œã†
try:
    client = genai.Client(api_key=api_key)
except Exception as e:
    st.error(f"Gemini Clientã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# --- ã‚¢ãƒ—ãƒªè¨­å®š ---
st.set_page_config(layout="wide")
st.title("ãŠç¥­ã‚Šæ¤œç´¢ï¼†è¨ªå•è¨˜éŒ²ã‚¢ãƒ—ãƒª")
geolocator = Nominatim(user_agent="japanese_festival_app_v11")

# (parse_event_period, get_lat_lonãªã©ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¯å¤‰æ›´ãªã—)
# --- Helper Functions ---
def parse_event_period(period_str):
    try:
        year_match = re.search(r'(\d{4})å¹´', period_str)
        month_match = re.search(r'(\d{1,2})æœˆ', period_str)
        if not year_match or not month_match: return None, None
        year, month = int(year_match.group(1)), int(month_match.group(1))
        days = [int(d) for d in re.findall(r'(\d{1,2})æ—¥', period_str)]
        if not days: return None, None
        start_date = datetime(year, month, days[0])
        end_date = datetime(year, month, days[-1])
        return start_date, end_date + timedelta(days=1)
    except (ValueError, IndexError):
        return None, None

@st.cache_data(ttl=3600)
def get_lat_lon(address):
    if not address or pd.isna(address): return None, None
    try:
        location = geolocator.geocode(address + ", Japan", timeout=10)
        if location: return location.latitude, location.longitude
        time.sleep(1)
    except Exception as e:
        st.warning(f"ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¤±æ•—: {address} - {e}")
    return None, None

@st.cache_data(ttl=600)
def get_worksheet_data(sheet_name):
    try:
        ws = get_or_create_worksheet(sheet_name)
        return pd.DataFrame(ws.get_all_records())
    except Exception as e:
        st.error(f"'{sheet_name}'ã‚·ãƒ¼ãƒˆèª­è¾¼å¤±æ•—: {e}")
        return pd.DataFrame()

def get_or_create_worksheet(sheet_name):
    spreadsheet = gc.open_by_key(SPREADSHEET_ID)
    try:
        return spreadsheet.worksheet(sheet_name)
    except gspread.WorksheetNotFound:
        ws = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
        headers = {
            "Festival_Data": ["Festival Name", "Location", "Event Period", "Description", "Recommended Highlights", "Latitude", "Longitude", "Added Date"],
            "Visit_Records": ["Festival Name", "Location", "Visit Date", "Photo Path", "Photo Latitude", "Photo Longitude", "Matched", "Distance_km", "Photo Count"]
        }
        if sheet_name in headers:
            ws.append_row(headers[sheet_name])
        return ws

# (get_gemini_recommendations, add_festivals_to_sheetãªã©ã¯å¤‰æ›´ãªã—)
def get_gemini_recommendations(query):
    """Gemini APIã§ãŠç¥­ã‚Šæƒ…å ±ã‚’å–å¾—ï¼ˆç·¯åº¦çµŒåº¦ã‚‚å«ã‚€ï¼‰"""
    try:
        config_with_search = types.GenerateContentConfig(
            tools=[types.Tool(google_search=types.GoogleSearch())],
            max_output_tokens=4096,
            temperature=0.0,
            top_k=40,
            top_p=1.0,
        )

        prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬ã®ãŠç¥­ã‚Šã‚’æ¤œç´¢ã™ã‚‹å„ªç§€ãªAIã§ã™ã€‚
æ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ "{query}" ã«åˆã†ãŠç¥­ã‚Šã‚’ã‚¦ã‚§ãƒ–ã§èª¿ã¹ã¦ã€**å¿…ãš**ä»¥ä¸‹ã®JSONå½¢å¼ã®ãƒªã‚¹ãƒˆã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

é‡è¦ï¼šå„ãŠç¥­ã‚Šã®æ­£ç¢ºãªä½æ‰€ã¨ç·¯åº¦çµŒåº¦ã‚’èª¿ã¹ã¦å«ã‚ã¦ãã ã•ã„ã€‚

ä»–ã®æ–‡ç« ãƒ»èª¬æ˜ãƒ»è£œè¶³æ–‡ãƒ»è£…é£¾ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãªã©ã¯**ä¸€åˆ‡è¿½åŠ ã—ãªã„ã§ãã ã•ã„**ã€‚
è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ [] ã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

[
  {{
    "Festival Name": "ä¸‰ç¤¾ç¥­",
    "Location": "æ±äº¬éƒ½å°æ±åŒºæµ…è‰2-3-1 æµ…è‰ç¥ç¤¾",
    "Event Period": "2025å¹´5æœˆ16æ—¥ï½18æ—¥",
    "Description": "ä¾‹å¤§ç¥­ã§ç¥è¼¿æ¸¡å¾¡ãŒè¦‹ã©ã“ã‚",
    "Recommended Highlights": "æœ¬ç¤¾ç¥è¼¿ã®å®®å‡ºã—ãƒ»å®®å…¥ã‚Š",
    "Latitude": 35.714844,
    "Longitude": 139.796707
  }}
]
"""

        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=config_with_search
        )
        
        return response.text

    except Exception as e:
        st.error(f"Gemini APIã¨ã®é€šä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None


def add_festivals_to_sheet(festivals_df):
    """ãŠç¥­ã‚Šãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ ã™ã‚‹é–¢æ•°"""
    ws = get_or_create_worksheet("Festival_Data")
    existing_df = get_worksheet_data("Festival_Data")
    new_rows = []
    
    for _, row in festivals_df.iterrows():
        is_duplicate = False
        if not existing_df.empty:
            is_duplicate = ((existing_df['Festival Name'] == row['Festival Name']) & 
                            (existing_df['Event Period'] == row['Event Period'])).any()
        
        if not is_duplicate:
            lat, lon = row.get('Latitude'), row.get('Longitude')
            if pd.isna(lat) or pd.isna(lon) or not lat or not lon:
                lat, lon = get_lat_lon(row['Location'])
            
            new_rows.append([
                row['Festival Name'], row['Location'], row['Event Period'],
                row['Description'], row.get('Recommended Highlights', ''),
                lat or "", lon or "", datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            ])
            
    if new_rows:
        ws.append_rows(new_rows)
        st.success(f"{len(new_rows)}ä»¶ã®æ–°ã—ã„ãŠç¥­ã‚Šã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
        st.cache_data.clear() # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        st.rerun() # ãƒšãƒ¼ã‚¸ã‚’å†å®Ÿè¡Œã—ã¦å³æ™‚åæ˜ 
    else:
        st.info("è¿½åŠ ã™ã‚‹æ–°ã—ã„ãŠç¥­ã‚Šã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚")


# (get_exif_data, get_gps_from_exif, haversine_distanceã¯å¤‰æ›´ãªã—)
def get_exif_data(image):
    exif_data = {}
    try:
        info = image.getexif()
        if not info: return {}
        for tag, value in info.items():
            decoded = TAGS.get(tag, tag)
            exif_data[decoded] = value
        gps_info_raw = info.get_ifd(0x8825)
        if gps_info_raw:
            exif_data['GPSInfo'] = {}
            for key, val in gps_info_raw.items():
                decode = GPSTAGS.get(key, key)
                exif_data['GPSInfo'][decode] = val
    except Exception: pass
    return exif_data
def get_gps_from_exif(exif_data):
    try:
        gps_info = exif_data.get('GPSInfo')
        if not gps_info: return None, None
        def convert_to_degrees(value):
            if isinstance(value, (list, tuple)) and len(value) == 3: return float(value[0]) + (float(value[1]) / 60.0) + (float(value[2]) / 3600.0)
            return None
        lat_data, lon_data = gps_info.get('GPSLatitude'), gps_info.get('GPSLongitude')
        lat_ref, lon_ref = gps_info.get('GPSLatitudeRef'), gps_info.get('GPSLongitudeRef')
        if lat_data and lon_data and lat_ref and lon_ref:
            lat, lon = convert_to_degrees(lat_data), convert_to_degrees(lon_data)
            if lat is None or lon is None: return None, None
            if lat_ref == 'S': lat = -lat
            if lon_ref == 'W': lon = -lon
            return lat, lon
    except Exception: pass
    return None, None
def haversine_distance(lat1, lon1, lat2, lon2):
    R, lat1_rad, lon1_rad, lat2_rad, lon2_rad = 6371.0, math.radians(lat1), math.radians(lon1), math.radians(lat2), math.radians(lon2)
    dlon, dlat = lon2_rad - lon1_rad, lat2_rad - lat1_rad
    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2
    return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))

# (find_closest_festivalã¯å¤‰æ›´ãªã—)
def find_closest_festival(photo_lat, photo_lon, photo_date, festival_data_df):
    min_distance = float('inf')
    closest = None
    for _, festival in festival_data_df.iterrows():
        try:
            fest_lat, fest_lon = float(festival.get('Latitude')), float(festival.get('Longitude'))
            distance = haversine_distance(photo_lat, photo_lon, fest_lat, fest_lon)
            if distance < min_distance and distance < 2.0:
                period_str = festival.get('Event Period')
                if period_str:
                    start_date, end_date = parse_event_period(period_str)
                    if start_date and end_date and start_date <= photo_date < end_date:
                        min_distance = distance
                        closest = {'name': festival['Festival Name'],'location': festival['Location'],'distance': round(distance, 2)}
        except (ValueError, TypeError, AttributeError): continue
    return closest

def process_drive_photos():
    """ã€æ”¹å–„ç‰ˆã€‘å†™çœŸã‚’è¨ªå•ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ã¾ã¨ã‚ã¦è¨˜éŒ²ã™ã‚‹"""
    try:
        # 1. æœªå‡¦ç†ã®å†™çœŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        visit_df = get_worksheet_data("Visit_Records")
        processed_photos = set(visit_df['Photo Path']) if not visit_df.empty else set()
        results = drive_service.files().list(q=f"'{DRIVE_FOLDER_ID}' in parents and mimeType contains 'image/'", fields="files(id, name)").execute()
        all_files = results.get('files', [])
        files_to_process = [f for f in all_files if f['name'] not in processed_photos]
        
        if not files_to_process:
            st.info("å‡¦ç†å¯¾è±¡ã®æ–°ã—ã„å†™çœŸã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []

        st.info(f"Google Driveã§{len(all_files)}å€‹ã®å†™çœŸã‚’ç™ºè¦‹ã€‚ã†ã¡{len(files_to_process)}å€‹ãŒæœªå‡¦ç†ã§ã™ã€‚")
        
        # 2. æœªå‡¦ç†å†™çœŸã®EXIFæƒ…å ±ã‚’ä¸€æ‹¬ã§æŠ½å‡º
        photo_details = []
        progress_bar = st.progress(0, text="å†™çœŸã®æƒ…å ±ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
        for i, file in enumerate(files_to_process):
            try:
                file_content = drive_service.files().get_media(fileId=file['id']).execute()
                image = Image.open(io.BytesIO(file_content))
                exif_data = get_exif_data(image)
                lat, lon = get_gps_from_exif(exif_data)
                if lat and lon:
                    photo_date_str = exif_data.get('DateTime', datetime.now().strftime("%Y:%m:%d %H:%M:%S"))
                    photo_details.append({
                        'file_name': file['name'],
                        'timestamp': datetime.strptime(photo_date_str, "%Y:%m:%d %H:%M:%S"),
                        'lat': lat, 'lon': lon
                    })
            except Exception as e:
                st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ ({file['name']}): {e}")
            progress_bar.progress((i + 1) / len(files_to_process))
        
        if not photo_details:
            st.info("GPSæƒ…å ±ä»˜ãã®æ–°ã—ã„å†™çœŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []

        # 3. å†™çœŸã‚’ã€Œè¨ªå•ã‚°ãƒ«ãƒ¼ãƒ—ã€ã«åˆ†é¡
        visit_groups = {}
        festival_df = get_worksheet_data("Festival_Data")
        for photo in photo_details:
            visit_date_str = photo['timestamp'].strftime("%Y-%m-%d")
            closest_festival = find_closest_festival(photo['lat'], photo['lon'], photo['timestamp'], festival_df)
            
            group_key = (closest_festival['name'], visit_date_str) if closest_festival else ('GPSè¨˜éŒ²ã®ã¿', visit_date_str)
            
            if group_key not in visit_groups:
                visit_groups[group_key] = {'festival_info': closest_festival, 'photos': []}
            visit_groups[group_key]['photos'].append(photo)

        # 4. å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ä»£è¡¨è¨˜éŒ²ã‚’ä½œæˆ
        new_visit_records = []
        for group_key, group_data in visit_groups.items():
            photos_in_group = group_data['photos']
            festival_info = group_data['festival_info']
            
            # æœ€ã‚‚æ—©ã„å†™çœŸã‚’ä»£è¡¨ã¨ã—ã¦é¸æŠ
            earliest_photo = min(photos_in_group, key=lambda p: p['timestamp'])
            
            if festival_info:
                record = {
                    'Festival Name': festival_info['name'], 'Location': festival_info['location'],
                    'Visit Date': group_key[1], 'Photo Path': earliest_photo['file_name'],
                    'Photo Latitude': earliest_photo['lat'], 'Photo Longitude': earliest_photo['lon'],
                    'Matched': 'Yes', 'Distance_km': festival_info['distance'],
                    'Photo Count': len(photos_in_group)
                }
            else: # GPSè¨˜éŒ²ã®ã¿ã®å ´åˆ
                record = {
                    'Festival Name': 'GPSè¨˜éŒ²ã®ã¿', 'Location': f"ç·¯åº¦{earliest_photo['lat']:.4f}, çµŒåº¦{earliest_photo['lon']:.4f}",
                    'Visit Date': group_key[1], 'Photo Path': earliest_photo['file_name'],
                    'Photo Latitude': earliest_photo['lat'], 'Photo Longitude': earliest_photo['lon'],
                    'Matched': 'No', 'Distance_km': None,
                    'Photo Count': len(photos_in_group)
                }
            new_visit_records.append(record)

        # 5. æ–°ã—ã„è¨ªå•è¨˜éŒ²ã‚’ã‚·ãƒ¼ãƒˆã«ä¸€æ‹¬ä¿å­˜
        if new_visit_records:
            visit_ws = get_or_create_worksheet("Visit_Records")
            headers = visit_ws.row_values(1)
            rows_to_append = [[r.get(h, '') for h in headers] for r in new_visit_records]
            visit_ws.append_rows(rows_to_append)
            total_photos_processed = sum(r['Photo Count'] for r in new_visit_records)
            st.success(f"{len(new_visit_records)}ä»¶ã®æ–°ã—ã„è¨ªå•ï¼ˆè¨ˆ{total_photos_processed}æšã®å†™çœŸï¼‰ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚")
            st.cache_data.clear() # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        else:
            st.info("è¨˜éŒ²ã™ã‚‹æ–°ã—ã„è¨ªå•ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
        return new_visit_records

    except Exception as e:
        st.error(f"Google Driveã®å†™çœŸå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# (åœ°å›³ã¨çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤ºéƒ¨åˆ†ã¯å¤‰æ›´ãªã—)
# --- åœ°å›³ã¨çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º ---
def create_map(df, lat_col, lon_col, popup_cols, tooltip_col, color):
    df = df.dropna(subset=[lat_col, lon_col]).copy()
    df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')
    df[lon_col] = pd.to_numeric(df[lon_col], errors='coerce')
    df = df.dropna(subset=[lat_col, lon_col])
    if df.empty: return None
    center = [df[lat_col].mean(), df[lon_col].mean()]
    m = folium.Map(location=center, zoom_start=10)
    for _, row in df.iterrows():
        popup_html = "<br>".join([f"<b>{col}:</b> {row[col]}" for col in popup_cols if col in row])
        folium.Marker([row[lat_col], row[lon_col]], popup=popup_html, tooltip=row[tooltip_col], icon=folium.Icon(color=color, icon='star')).add_to(m)
    return m

def create_visit_heatmap(visit_df):
    visit_df = visit_df.dropna(subset=['Photo Latitude', 'Photo Longitude']).copy()
    visit_df['Photo Latitude'] = pd.to_numeric(visit_df['Photo Latitude'], errors='coerce')
    visit_df['Photo Longitude'] = pd.to_numeric(visit_df['Photo Longitude'], errors='coerce')
    visit_df = visit_df.dropna(subset=['Photo Latitude', 'Photo Longitude'])
    if visit_df.empty: return None, 0
    heat_data = [[row['Photo Latitude'], row['Photo Longitude']] for _, row in visit_df.iterrows()]
    center = [visit_df['Photo Latitude'].mean(), visit_df['Photo Longitude'].mean()]
    m = folium.Map(location=center, zoom_start=10)
    HeatMap(heat_data, radius=15).add_to(m)
    return m, len(heat_data)

def display_visit_statistics(festival_df, visit_df):
    """ã€ä¿®æ­£ã€‘è¨ªå•çŠ¶æ³ã®çµ±è¨ˆæƒ…å ±ã‚’4åˆ—ã§è¡¨ç¤ºã—ã€è¨ªå•ç‡ã‚’å¾©æ´»"""
    st.subheader("ğŸ“Š è¨ªå•çŠ¶æ³ã‚µãƒãƒªãƒ¼")
    total_festivals = len(festival_df)
    if visit_df.empty:
        visited_count, total_photos = 0, 0
    else:
        visited_festivals = visit_df[visit_df['Matched'] == 'Yes']
        visited_count = visited_festivals['Festival Name'].nunique() if not visited_festivals.empty else 0
        total_photos = pd.to_numeric(visit_df['Photo Count'], errors='coerce').sum()
    visit_rate = (visited_count / total_festivals * 100) if total_festivals > 0 else 0
    
    # â˜…ä¿®æ­£ç‚¹: åˆ—ã‚’4ã¤ã«ã—ã€è¨ªå•ç‡ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’å†è¿½åŠ 
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ç·ãŠç¥­ã‚Šç™»éŒ²æ•°", f"{total_festivals} ä»¶")
    col2.metric("è¨ªå•æ¸ˆã¿ãŠç¥­ã‚Šæ•°", f"{visited_count} ä»¶", help="ãƒãƒƒãƒãƒ³ã‚°ã«æˆåŠŸã—ãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãŠç¥­ã‚Šã®æ•°")
    col3.metric("ãŠç¥­ã‚Šè¨ªå•ç‡", f"{visit_rate:.1f} %")
    col4.metric("ç·æ’®å½±æšæ•°", f"{int(total_photos)} æš", help="è¨ªå•è¨˜éŒ²ã«ã‚ã‚‹å†™çœŸã®åˆè¨ˆæšæ•°")


# --- Streamlit UI ---
st.header("ğŸ“‹ ç™»éŒ²æ¸ˆã¿ãŠç¥­ã‚Šä¸€è¦§")
festival_df = get_worksheet_data("Festival_Data")
if not festival_df.empty:
    st.dataframe(festival_df, use_container_width=True)
else:
    st.info("ã¾ã ãŠç¥­ã‚ŠãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

st.header("ğŸŒ ãŠç¥­ã‚Šã‚’æ¢ã™")
query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šæ±äº¬ 7æœˆã€ç¥ç”°ç¥­ï¼‰")
if st.button("ğŸ” æ¤œç´¢"):
    if query:
        with st.spinner("GeminiãŒãŠç¥­ã‚Šã‚’æ¤œç´¢ä¸­..."):
            raw_response = get_gemini_recommendations(query)
            try:
                json_match = re.search(r'\[.*\]', raw_response, re.DOTALL)
                if json_match:
                    festivals = json.loads(json_match.group(0))
                    if festivals:
                        add_festivals_to_sheet(pd.DataFrame(festivals))
                    else:
                        st.info("è©²å½“ã™ã‚‹ãŠç¥­ã‚ŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    st.error("æ¤œç´¢çµæœã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚")
            except json.JSONDecodeError:
                st.error("æ¤œç´¢çµæœã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.code(raw_response)
    else:
        st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

st.header("ğŸ—ºï¸ ãŠç¥­ã‚Šãƒãƒƒãƒ—")
if not festival_df.empty:
    festival_map = create_map(festival_df, 'Latitude', 'Longitude', ['Festival Name', 'Location', 'Event Period'], 'Festival Name', 'red')
    if festival_map:
        st.components.v1.html(festival_map.get_root().render(), height=500)
    else:
        st.warning("åœ°å›³ã«è¡¨ç¤ºã§ãã‚‹ãŠç¥­ã‚ŠãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    st.info("è¡¨ç¤ºã™ã‚‹ãŠç¥­ã‚Šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.header("ğŸ“ è¨ªå•è¨˜éŒ²")
if st.button("ğŸ“· Google Driveã®å†™çœŸã‚’å‡¦ç†ã—ã¦è¨ªå•è¨˜éŒ²ã‚’æ›´æ–°"):
    process_drive_photos()
    st.rerun() # å‡¦ç†å¾Œã«ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦æœ€æ–°ã®è¨˜éŒ²ã¨çµ±è¨ˆã‚’è¡¨ç¤º

visit_df = get_worksheet_data("Visit_Records")
if not visit_df.empty:
    # â˜…â˜…â˜… ã‚¨ãƒ©ãƒ¼å¯¾ç­–: Distance_kmåˆ—ã‚’å¼·åˆ¶çš„ã«æ•°å€¤ã«å¤‰æ› â˜…â˜…â˜…
    # ç©ºæ–‡å­—''ãªã©ã€æ•°å€¤ã«ã§ããªã„å€¤ã¯NaNï¼ˆNot a Numberï¼‰ã«å¤‰æ›ã•ã‚Œã€ã‚¨ãƒ©ãƒ¼ã‚’é˜²ãã¾ã™ã€‚
    visit_df['Distance_km'] = pd.to_numeric(visit_df['Distance_km'], errors='coerce')

    st.dataframe(visit_df.style.format({
        'Photo Latitude': '{:.6f}',
        'Photo Longitude': '{:.6f}',
        'Distance_km': '{:.2f}' # è·é›¢ã‚‚å°æ•°ç‚¹2æ¡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    }), use_container_width=True)
else:
    st.info("ã¾ã è¨ªå•è¨˜éŒ²ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.header("ğŸ”¥ è¨ªå•æ¸ˆã¿ã‚¨ãƒªã‚¢ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
display_visit_statistics(festival_df, visit_df)
hmap, count = create_visit_heatmap(visit_df)
if hmap:
    st.success(f"å…¨ {count} åœ°ç‚¹ã®è¨ªå•è¨˜éŒ²ã‹ã‚‰ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
    st.components.v1.html(hmap.get_root().render(), height=500)
else:
    st.info("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®è¨ªå•è¨˜éŒ²ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")